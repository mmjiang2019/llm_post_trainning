# llm_post_trainning
# Model Training Process:
Randomly Initialied Model ----pre-training----> 
Base Model ----post-training----> 
Instruct/Chat Model ----(continuous) post-training---> 
Customized Model

## Base Model
predict next word or token based on the context after learning from a large corpus of text data.

## Instruct/Chat Model
respond to instructions after learning responses from curated dataset.

## Customized Model
Specialized in certain domain or have specific behaviors after changing behaviors or enhancing capabilities.

# Methods of Post-Traing
## Supervised Fine-Tuning
Target of SFT

### Jumpstarting new model behavior

Pre-trained Models ----> Instruct Models

Non-reasoning Models ----> Reasoning Models

Let the models use certain tools without providing tool descriptions in prompts.

### Improving model capabilities

distillation capabilities for small models by training on high-quality synthetic data generated by larger models.

### Common Methods for High-Quality SFT data curation
#### Knowledge Distillation
Generate responses from larger instruct models
#### Best of K/rejection sampling
Generate multiple responses from the original models and select the best one among them.
#### Filtering
Start from larger scale SFT dataset, filtering according to the quality of responses and diversity of the prompts.

### Principles of SFT
#### Quality > Quantity
1000 high-quality, diverse data works better than 1000000 mixed-quality data on improvement of SFT.
## Direct Preference Optimization
## Online Reinforcement Learning

# Full Fine-Tunning VS Parameter Efficient Fine-Tunning(PEFT)
Both of them can be used in any of the post-training methods.
And PEFT such as LoRa saves memery, learns less while forget less.