# llm_post_trainning
# Model Training Process:
Randomly Initialied Model ----pre-training----> 
Base Model ----post-training----> 
Instruct/Chat Model ----(continual) post-training---> 
Customized Model

## Base Model
predict next word or token based on the context after learning from a large corpus of text data.

## Instruct/Chat Model
respond to instructions after learning responses from curated dataset.

## Customized Model
Specialized in certain domain or have specific behaviors after changing behaviors or enhancing capabilities.

# Methods of Post-Traing

## Supervised Fine-Tuning
Target of SFT

### Jumpstarting new model behavior

Pre-trained Models ----> Instruct Models

Non-reasoning Models ----> Reasoning Models

Let the models use certain tools without providing tool descriptions in prompts.

### Improving model capabilities

distillation capabilities for small models by training on high-quality synthetic data generated by larger models.

### Common Methods for High-Quality SFT data curation
#### Knowledge Distillation
Generate responses from larger instruct models
#### Best of K/rejection sampling
Generate multiple responses from the original models and select the best one among them.
#### Filtering
Start from larger scale SFT dataset, filtering according to the quality of responses and diversity of the prompts.

### Principles of SFT
#### Quality > Quantity
1000 high-quality, diverse data works better than 1000000 mixed-quality data on improvement of SFT.

## Direct Preference Optimization
DPO minimizes the contrasive loss which penalizes negative response and encourages positive respons.

DPO loss is a cross entropy loss on the reward difference of a "re-parameterized" reward function.

### Best Use Case for DPO
#### Changing model behavior
##### Making small modifications of model responses
- Identity
- Multilingual
- Instruction following
- Safety

#### Improving model capabilities
- Better than SFT in improving model capabilities due to contrastive nature.
- Online DPO is better for improving model capabilities than offline DPO.

### Principles of DPO Data Curation
#### Correnction
- Generate responses from original model as negative, make enhancements as positive responses.
    - Example: I'm Llama (Negative) ----> I'm Athene (Positive)
#### Online/On-Policy
Your positive & negative examples can both come from your model's distribution. One may generate multiple responses from the current model for the same prompt, and collect the best response as positive sample and the worst response as negative sample.
- One can choose best/worst response based on reward functions / human judgement.

#### Avoid Overfitting
- DPO is doing reward learning with can easily overfit to some shortcut when the preferred answers have shortcuts to learn compared with the non-preferred answers.
    - Example: when positive sample always contains a few special words while negative samples do not.

## Online RL(Reinforcement Learning)
Let Model Explore Better Responses by itself.

Batch of Prompts ----> 

Language Model --generate--> 

Prompts, Responses ----> 

Reward Function --Label--> 

Prompts,Responses,Rewards

### Reward Function
#### Option 1: Trained Reward Model
One post with two summaries judged by a human are fed to the reward model.

The reward model calculates a reward r for earch summary.

The loss is calculated based on the rewards and human label, and is used to update the reeard model.

#### Option 2: Verifiable Reward
##### Math
Check if the response matches the ground truth.

##### Coding
Running unit tests.

##### Summary of the two
- requires preparation of the ground truth for math, unit tests for coding, or sandbox execution
- More reliable than reward model in those domains
- Used more often for training reasoning models


### Methods for Online RL
#### Policy Training in Online RL
##### Proximal Policy Optimization (PPO)
近端策略优化（PPO）是一种强化学习算法，用于训练策略以最大化预期回报。它通过使用一个旧策略和一个新策略之间的近似梯度来更新策略参数，从而保持新旧策略的相似性，并减少对新策略的过度拟合风险。

最先由 ChatGPT 提出，并被广泛用于各种强化学习任务中。

##### Group Relative Policy Optimization (GRPO)
不再需要像PPO那样加入额外的价值函数近似，而是直接使用多个采样输出的平均奖励作为Baseline，显著减少了训练资源的使用。

DeepSeek-R1 核心强化学习算法

#### GRPO VS PPO
##### Common Points
- Both are very effective online RL algorithms!

##### GRPO
- Well-suited for binary (often correctness-based) reward
- Requres larger amount of samples
- Requires less GPU memory (no value model needed)

##### PPO
- Works well with reward model or binary reward
- More sample efficient with a well-trained value model
- Requires more GPU memory (value model)

# Full Fine-Tunning VS Parameter Efficient Fine-Tunning(PEFT)
Both of them can be used in any of the post-training methods.
And PEFT such as LoRa saves memery, learns less while forget less.