# llm_post_trainning
# Model Training Process:
Randomly Initialied Model ----pre-training----> 
Base Model ----post-training----> 
Instruct/Chat Model ----(continual) post-training---> 
Customized Model

## Base Model
predict next word or token based on the context after learning from a large corpus of text data.

## Instruct/Chat Model
respond to instructions after learning responses from curated dataset.

## Customized Model
Specialized in certain domain or have specific behaviors after changing behaviors or enhancing capabilities.

# Methods of Post-Traing
## Supervised Fine-Tuning
Target of SFT

### Jumpstarting new model behavior

Pre-trained Models ----> Instruct Models

Non-reasoning Models ----> Reasoning Models

Let the models use certain tools without providing tool descriptions in prompts.

### Improving model capabilities

distillation capabilities for small models by training on high-quality synthetic data generated by larger models.

### Common Methods for High-Quality SFT data curation
#### Knowledge Distillation
Generate responses from larger instruct models
#### Best of K/rejection sampling
Generate multiple responses from the original models and select the best one among them.
#### Filtering
Start from larger scale SFT dataset, filtering according to the quality of responses and diversity of the prompts.

### Principles of SFT
#### Quality > Quantity
1000 high-quality, diverse data works better than 1000000 mixed-quality data on improvement of SFT.

## Direct Preference Optimization
DPO minimizes the contrasive loss which penalizes negative response and encourages positive respons.

DPO loss is a cross entropy loss on the reward difference of a "re-parameterized" reward function.

### Best Use Case for DPO
#### Changing model behavior
##### Making small modifications of model responses
- Identity
- Multilingual
- Instruction following
- Safety

#### Improving model capabilities
- Better than SFT in improving model capabilities due to contrastive nature.
- Online DPO is better for improving model capabilities than offline DPO.

### Principles of DPO Data Curation
#### Correnction
- Generate responses from original model as negative, make enhancements as positive responses.
    - Example: I'm Llama (Negative) ----> I'm Athene (Positive)
#### Online/On-Policy
Your positive & negative examples can both come from your model's distribution. One may generate multiple responses from the current model for the same prompt, and collect the best response as positive sample and the worst response as negative sample.
- One can choose best/worst response based on reward functions / human judgement.

#### Avoid Overfitting
- DPO is doing reward learning with can easily overfit to some shortcut when the preferred answers have shortcuts to learn compared with the non-preferred answers.
    - Example: when positive sample always contains a few special words while negative samples do not.

## Online Reinforcement Learning

# Full Fine-Tunning VS Parameter Efficient Fine-Tunning(PEFT)
Both of them can be used in any of the post-training methods.
And PEFT such as LoRa saves memery, learns less while forget less.